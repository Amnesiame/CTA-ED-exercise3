
# Preparion
```{r, echo=F}
library(kableExtra)
library(readr) # more informative and easy way to import data
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textstats) # for estimating similarity and complexity measures
library(stringdist) # for basic character-based distance measures
library(dplyr) #for wrangling data
library(tibble) #for wrangling data
library(ggplot2) #for visualization
```


# Original Example
```{r}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))

head(tweets)
unique(tweets$username)
length(unique(tweets$username))

## Generate document feature matrix

#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "tweet")

#add in username document-level information
docvars(tweets_corpus, "username") <- tweets$username

tweets_corpus

dfmat <- dfm(tokens(tweets_corpus),
             remove_punct = TRUE, 
             remove = stopwords("english"))

dfmat
 

## Compare between MPs

corrmat <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "correlation")

corrmat[1:5,1:5]


## Compare between measures


#estimate similarity, grouping by username

cos_sim <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "cosine") #specify method here as character object

cosmat <- as.matrix(cos_sim) #convert to a matrix

#generate data frame keeping only the row for Theresa May
cosmatdf <- as.data.frame(cosmat[23, c(1:22, 24)])

#rename column
colnames(cosmatdf) <- "corr_may"
  
#create column variable from rownames
cosmatdf <- tibble::rownames_to_column(cosmatdf, "username")

ggplot(cosmatdf) +
  geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Cosine similarity score") + 
  theme_minimal()



#specify different similarity measures to explore
methods <- c("correlation", "cosine", "dice", "edice")

#create empty dataframe
testdf_all <- data.frame()

#gen for loop across methods types
for (i in seq_along(methods)) {
  
  #pass method to character string object
  sim_method <- methods[[i]]
  
  #estimate similarity, grouping by username
  test <- dfmat %>%
    dfm_group(groups = username) %>%
    textstat_simil(margin = "documents", method = sim_method) #specify method here as character object created above
  
  testm <- as.matrix(test) #convert to a matrix
  
  #generate data frame keeping only the row for Theresa May
  testdf <- as.data.frame(testm[23, c(1:22, 24)])
  
  #rename column
  colnames(testdf) <- "corr_may"
  
  #create column variable from rownames
  testdf <- tibble::rownames_to_column(testdf, "username")
  
  #record method in new column variable
  testdf$method <- sim_method

  #bind all together
  testdf_all <- rbind(testdf_all, testdf)  
  
}

#create variable (for viz only) that is mean of similarity scores for each MP
testdf_all <- testdf_all %>%
  group_by(username) %>%
  mutate(mean_sim = mean(corr_may))

ggplot(testdf_all) +
  geom_point( aes(x=reorder(username, -mean_sim), y= corr_may, color = method)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Similarity score") + 
  theme_minimal()


## Complexity


speeches  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/speeches.rds?raw=true")))


head(speeches)

speeches$flesch.kincaid <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")

# returned as quanteda data.frame with document-level information;
# need just the score:
speeches$flesch.kincaid <- speeches$flesch.kincaid$Flesch.Kincaid

#get mean and standard deviation of Flesch-Kincaid, and N of speeches for each speaker
sum_corpus <- speeches %>%
  group_by(speaker) %>%
  summarise(mean = mean(flesch.kincaid, na.rm=TRUE),
                   SD=sd(flesch.kincaid, na.rm=TRUE),
                   N=length(speaker))

# calculate standard errors and confidence intervals
sum_corpus$se <- sum_corpus$SD / sqrt(sum_corpus$N)
sum_corpus$min <- sum_corpus$mean - 1.96*sum_corpus$se
sum_corpus$max <- sum_corpus$mean + 1.96*sum_corpus$se

sum_corpus

ggplot(sum_corpus, aes(x=speaker, y=mean)) +
  geom_bar(stat="identity") + 
  geom_errorbar(ymin=sum_corpus$min,ymax=sum_corpus$max, width=.2) +
  coord_flip() +
  xlab("") +
  ylab("Mean Complexity") + 
  theme_minimal() + 
  ylim(c(0,20))

```


# Exercise 1
# Compute distance measures such as "euclidean" or "manhattan" for the MP tweets as above, comparing between tweets by MPs and tweets by PM, Theresa May. 
```{r}
# Data
tweets <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))

# Matrix
tweets_corpus <- corpus(tweets, text_field = "tweet")
docvars(tweets_corpus, "username") <- tweets$username
dfmat <- dfm(tokens(tweets_corpus), remove_punct = TRUE, remove = stopwords("english"))

# Measures
methods <- c("euclidean", "manhattan")

# For
distdf_all <- data.frame()

for (i in seq_along(methods)) {
  dist_method <- methods[[i]]
  dist <- dfmat %>%
    dfm_group(groups = username) %>%
    textstat_dist(margin = "documents", method = dist_method)
  distm <- as.matrix(dist)
  distdf <- as.data.frame(distm[23, c(1:22, 24)])
  colnames(distdf) <- "dist_may"
  distdf <- tibble::rownames_to_column(distdf, "username")
  distdf$method <- dist_method
  distdf_all <- rbind(distdf_all, distdf)  
}

# Plot
ggplot(distdf_all) +
  geom_point(aes(x=reorder(username, -dist_may), y=dist_may, color=method)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Distance score") +
  theme_minimal()

```

# Exercise 2
# Estimate at least three other complexity measures for the EU speeches as above. Consider how the results compare to the Flesch-Kincaid measure used in the article by @schoonvelde_liberals_2019.

```{r}
# Data
speeches <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))

# Format
speeches$text <- as.character(speeches$text)

# Flesch-Kincaid 
fk_scores <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")
speeches$flesch_kincaid <- fk_scores$Flesch.Kincaid

# Dale-Chall
dc_scores <- textstat_readability(speeches$text, measure = "Dale.Chall")
speeches$dale_chall <- dc_scores$Dale.Chall

# Coleman-Liau
cl_scores <- textstat_readability(speeches$text, measure = "Coleman.Liau")
speeches$coleman_liau <- cl_scores$Coleman.Liau

# SMOG
smog_scores <- textstat_readability(speeches$text, measure = "SMOG")
speeches$smog <- smog_scores$SMOG

# Average + Standard deviations + sample sizes + 95% confidence intervals
sum_corpus_complexity <- speeches %>%
  group_by(speaker) %>%
  summarise(mean_fk = mean(flesch_kincaid, na.rm=TRUE),
            mean_dc = mean(dale_chall, na.rm=TRUE),
            mean_cl = mean(coleman_liau, na.rm=TRUE),
            mean_smog = mean(smog, na.rm=TRUE),
            SD_fk = sd(flesch_kincaid, na.rm=TRUE),
            SD_dc = sd(dale_chall, na.rm=TRUE),
            SD_cl = sd(coleman_liau, na.rm=TRUE),
            SD_smog = sd(smog, na.rm=TRUE),
            N = n()) %>%
  mutate(se_fk = SD_fk / sqrt(N),
         min_fk = mean_fk - 1.96 * se_fk,
         max_fk = mean_fk + 1.96 * se_fk)

# Plot Flesch-Kincaid
ggplot(speeches, aes(x=flesch_kincaid)) +
  geom_histogram(binwidth = 1, fill="blue", color="black") +
  theme_minimal() +
  ggtitle("Flesch-Kincaid Grade Level Distribution") +
  xlab("Flesch-Kincaid Grade Level") +
  ylab("Frequency")

# Plot Dale-Chall
ggplot(speeches, aes(x=dale_chall)) +
  geom_histogram(binwidth = 0.5, fill="yellow", color="black") +
  theme_minimal() +
  ggtitle("Dale-Chall Readability Formula Distribution") +
  xlab("Dale-Chall Score") +
  ylab("Frequency")

# Plot Coleman-Liau
ggplot(speeches, aes(x=coleman_liau)) +
  geom_histogram(binwidth = 0.5, fill="red", color="black") +
  theme_minimal() +
  ggtitle("Coleman-Liau Index Distribution") +
  xlab("Coleman-Liau Index") +
  ylab("Frequency")

# Plot SMOG
ggplot(speeches, aes(x=smog)) +
  geom_histogram(binwidth = 0.5, fill="green", color="black") +
  theme_minimal() +
  ggtitle("SMOG Index Distribution") +
  xlab("SMOG Index") +
  ylab("Frequency")

```

# Exercise 3 (Advanced---optional) 
# Estimate similarity scores between the MP tweets and the PM tweets for each week contained in the data. Plot the results. 

In process.
